{"cells":[{"cell_type":"markdown","metadata":{"id":"nK320JzylP9J"},"source":["# CSCI 390: Machine Learning\n","\n","File name: Turauth_HW3.ipynb\n","\n","The purpose of this file is to explore the application of linear regression and Support Vector Machines."]},{"cell_type":"markdown","metadata":{"id":"oz29lmrHXqAD"},"source":["## Part 1"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27338,"status":"ok","timestamp":1735433899978,"user":{"displayName":"JONATHAN WOYCHUK","userId":"03015250595842019482"},"user_tz":300},"id":"uHTJ_oIDIX2v","outputId":"c3aab25b-d5c7-417c-ae04-4acb3743e829"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive to this notebook.\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1735434003146,"user":{"displayName":"JONATHAN WOYCHUK","userId":"03015250595842019482"},"user_tz":300},"id":"xtOPye1x8rCH","outputId":"9b48853b-5b5f-4f6e-a30d-84fa093cf375"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.606\n","['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[ 1.30559051,  0.09922117, -3.12146391,  4.48662399, -0.02925217,\n","        -0.97016368, -0.81743339, -0.89948456]])"]},"metadata":{},"execution_count":3}],"source":["# Import libraries.\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.preprocessing import MinMaxScaler # (https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import accuracy_score\n","import sklearn.model_selection as model\n","import sklearn.neighbors as nbrs\n","import sklearn.feature_selection as featsel\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt # https://medium.com/@basumatary18/implementing-linear-regression-on-california-housing-dataset-378e14e421b7\n","\n","# Import the data set (https://medium.com/@basumatary18/implementing-linear-regression-on-california-housing-dataset-378e14e421b7) and assign the features and target\n","# to X and y.\n","housing = fetch_california_housing(as_frame=True)\n","X = housing.data\n","y = housing.target.to_numpy().reshape(-1,1)\n","\n","# There is no need to check for data errors or anomalies or to examine and replace missing values because the dataset is well-known. There are no computed fields\n","# being considered in this exercise.\n","\n","# Normalize the data.\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","y = scaler.fit_transform(y)\n","\n","#Partition the data into training and test data.\n","X_train, X_test, y_train, y_test = model.train_test_split(X, y, test_size=.3, random_state=0)\n","\n","# Make a linear regressor, provide the final accuracy, and print out the coefficient.\n","reg = LinearRegression().fit(X, y)\n","print(round(reg.score(X, y),3))\n","print(housing.feature_names)\n","reg.coef_"]},{"cell_type":"markdown","source":["The coefficients indicate how great the effect of each feature is on the target, the housing price.\n","The greater the coefficient, the greater is the effect of the corresponding on the target.\n","Features AveRooms and AveBedrms have a greater effect on the target than the other features.\n","Features AveRooms, Population, AveOccup, Latitude, and Longitude are inversely proportional to\n","the target."],"metadata":{"id":"7DAv_DB07aOv"}},{"cell_type":"markdown","metadata":{"id":"PlqvD7hhF08L"},"source":["## Part 2"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SETqTj1Ne9I3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735434137338,"user_tz":300,"elapsed":1262,"user":{"displayName":"JONATHAN WOYCHUK","userId":"03015250595842019482"}},"outputId":"791eb42a-13ee-4d06-bd52-7f94fd308217"},"outputs":[{"output_type":"stream","name":"stdout","text":["The linear SVM score is 0.897059\n","The poly SVM score is 0.970588\n","The rbf SVM score is 0.911765\n","The sigmoid SVM score is 0.897059\n"]}],"source":["# Import libraries.\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import MinMaxScaler\n","import sklearn.model_selection as model\n","from sklearn.svm import SVC\n","from scipy import stats\n","import pandas as pd\n","import numpy as np\n","\n","# Show the file where to find the weather data, read it, and drop rows where the target value is missing.\n","pathName = \"/content/drive/MyDrive/Colab Notebooks/HW3/\"\n","df1 = pd.read_excel(pathName + 'weatherAUS100.xlsx', sheet_name='Data')\n","df2 = pd.read_excel(pathName + 'weatherAUS100.xlsx', sheet_name='Bounds')\n","df1.dropna(subset='RainToday',inplace=True)\n","X = df1.drop(['RainToday'], axis=1)\n","y = df1.RainToday\n","\n","# Remove data errors and anomalies from the feature data.\n","X2 = df2.drop(['Stats'], axis=1)\n","X2.to_numpy()\n","for feature, item in X.items(): # https://note.nkmk.me/en/python-pandas-dataframe-for-iteration/\n","  # Access bounds using .loc with column name and index label.\n","  lower_bound = X2.loc[2, feature]\n","  upper_bound = X2.loc[3, feature]\n","  # Replace values outside bounds with NaN.\n","  X.loc[X[feature] < lower_bound, feature] = np.nan\n","  X.loc[X[feature] > upper_bound, feature] = np.nan\n","\n","# Impute missing feature values.\n","imputer = SimpleImputer(strategy='mean')\n","X = imputer.fit_transform(X)\n","\n","# Normalize the data and partition it into training and testing data.\n","y.to_numpy().reshape(-1,1)\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","\n","X_train, X_test, y_train, y_test = model.train_test_split(X, y, test_size=.3, random_state=0)\n","\n","# Train and measure the performance of the classifier.\n","for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n","  clf = SVC(kernel=kernel).fit(X_train, y_train)\n","  print('The ' + kernel + ' SVM score is %f' % clf.score(X_train, y_train))"]},{"cell_type":"markdown","source":["The above version uses only the first hundred rows of the dataset. When using the full dataset, the rbf kernel was found to be the most accurate."],"metadata":{"id":"YFBUXMZC8APv"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n","\n","# Show the file where to find the weather data, read it, and drop rows where the target value is\n","pathName = \"/content/drive/MyDrive/Colab Notebooks/HW3/\"\n","df1 = pd.read_excel(pathName + 'weatherAUS1200.xlsx', sheet_name='Data')\n","df2 = pd.read_excel(pathName + 'weatherAUS1200.xlsx', sheet_name='Bounds')\n","df1.dropna(subset='RainToday',inplace=True)\n","X = df1.drop(['RainToday'], axis=1)\n","y = df1.RainToday\n","\n","# Remove data errors and anomalies from the feature data.\n","X2 = df2.drop(['Stats'], axis=1)\n","X2.to_numpy()\n","for feature, item in X.items(): # https://note.nkmk.me/en/python-pandas-dataframe-for-iteration/\n","  # Access bounds using .loc with column name and index label.\n","  lower_bound = X2.loc[2, feature]\n","  upper_bound = X2.loc[3, feature]\n","  # Replace values outside bounds with NaN.\n","  X.loc[X[feature] < lower_bound, feature] = np.nan\n","  X.loc[X[feature] > upper_bound, feature] = np.nan\n","\n","# Impute missing feature values.\n","imputer = SimpleImputer(strategy='mean')\n","X = imputer.fit_transform(X)\n","\n","# Normalize the data.\n","y.to_numpy().reshape(-1,1)\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","\n","# Use GridSearchCV to find the values of C and gamma that give the highest accuracy.\n","C_range = np.logspace(-2, 10, 13)\n","gamma_range = np.logspace(-9, 3, 13)\n","param_grid = dict(gamma=gamma_range, C=C_range)\n","cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n","grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n","grid.fit(X, y)\n","\n","print(\n","\"The best parameters are %s with a score of %0.2f.\"\n","% (grid.best_params_, grid.best_score_)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"collapsed":true,"id":"IjAZTUhc8T5M","executionInfo":{"status":"error","timestamp":1735434491570,"user_tz":300,"elapsed":86996,"user":{"displayName":"JONATHAN WOYCHUK","userId":"03015250595842019482"}},"outputId":"dea6fe26-d90e-481e-db3e-d5ec911e0036"},"execution_count":5,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-fe445e987c25>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m print(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    971\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["My latest version was only saved as a PDF.\n","\n","The output of the above code was this: \"\n","The best parameters are {'C': 10000.0, 'gamma': 0.1} with a score of 0.90.\"\n","\n","The above accuracy score is for the subsample including the first 1200 rows of the data."],"metadata":{"id":"4S5mGgp588t_"}},{"cell_type":"code","source":["# Import libraries.\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import MinMaxScaler\n","import sklearn.model_selection as model\n","from sklearn.svm import SVC\n","from scipy import stats\n","import pandas as pd\n","import numpy as np\n","\n","# Show the file where to find the weather data, read it, and drop rows where the target value is\n","pathName = \"/content/drive/MyDrive/Colab Notebooks/HW3/\"\n","df1 = pd.read_excel(pathName + 'weatherAUS.xlsx', sheet_name='Data')\n","df2 = pd.read_excel(pathName + 'weatherAUS.xlsx', sheet_name='Bounds')\n","df1.dropna(subset='RainToday',inplace=True)\n","X = df1.drop(['RainToday'], axis=1)\n","y = df1.RainToday\n","\n","# Remove data errors and anomalies from the feature data.\n","X2 = df2.drop(['Stats'], axis=1)\n","X2.to_numpy()\n","for feature, item in X.items(): # https://note.nkmk.me/en/python-pandas-dataframe-for-iteration/\n","  # Access bounds using .loc with column name and index label.\n","  lower_bound = X2.loc[2, feature]\n","  upper_bound = X2.loc[3, feature]\n","\n","# Replace values outside bounds with NaN.\n","X.loc[X[feature] < lower_bound, feature] = np.nan\n","X.loc[X[feature] > upper_bound, feature] = np.nan\n","\n","# Impute missing feature values.\n","imputer = SimpleImputer(strategy='mean')\n","X = imputer.fit_transform(X)\n","\n","# Normalize the data and partition it into training and testing data.\n","y.to_numpy().reshape(-1,1)\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","X_train, X_test, y_train, y_test = model.train_test_split(X, y, test_size=.3, random_state=0)\n","clf = SVC(C=10000, kernel='rbf', gamma=0.1).fit(X_train, y_train)\n","\n","# Train and measure the performance of the classifier using the best parameters for the rbf kerne\n","print('The best rbf setting SVM score for the entire model is %f' % clf.score(X_train, y_train))"],"metadata":{"id":"T0iHnuLo9YNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A few lines of this last section were cut off in the PDF, but I filled them in and am pretty sure the code shown gave the following output: \"The best rbf setting SVM score for the entire model is 0.836858\"."],"metadata":{"id":"qWQXJ0Bd-UjV"}}],"metadata":{"colab":{"provenance":[{"file_id":"19czvrGET8BQJFnd-bFaRy-WInzfyJo8Y","timestamp":1728435330365}],"authorship_tag":"ABX9TyNAY/E1olfA5nChJcoIzdzg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}